{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5216070a-8153-46af-be74-9fa32d955eb2",
   "metadata": {},
   "source": [
    "## Table of Contents - **Data Preparation**\n",
    "\n",
    "  **STEP 1:** Clean all numerical features and the target variable price so that they can be used in training algorithms (as a regular feature).\n",
    "  \n",
    "  **STEP 2:** Clean all features that contains multiple items of information, e.g. creating email, phone, work_email, etc. from feature host_verifications.\n",
    "  \n",
    "  **STEP 3:** Impute missing values for all features in both training and test datasets. \n",
    "  \n",
    "  **STEP 4:** Encode all categorical variables appropriately.\n",
    "  \n",
    "  **STEP 5:** Create dummy variables on amenities feature. \n",
    "\n",
    "  **STEP 6:** Perform exploratory data analysis to measure the relationship between the features and the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b384ac48-8997-4518-a248-72de599ef620",
   "metadata": {},
   "source": [
    "**STEP 1: Clean all numerical features and the target variable price so that they can be used in training algorithms (as a regular feature).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697a43c7-6814-45c4-9f33-ffb74b71394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove % sign for host_response_rate and host_acceptance_rate\n",
    "df['host_response_rate'] = df['host_response_rate'].str.replace('%', '').astype(float) / 100\n",
    "df['host_acceptance_rate'] = df['host_acceptance_rate'].str.replace('%', '').astype(float) / 100\n",
    "\n",
    "# remove $ sign for price target variable\n",
    "df['price'] = df['price'].str.replace('[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "# convert the string half/Half to a number 0.5\n",
    "df['bathrooms'] = df['bathrooms'].str.replace('half', '0.5 half')\n",
    "df['bathrooms'] = df['bathrooms'].str.replace('Half', '0.5 half')\n",
    "df['Bath Type'] = df['bathrooms'].astype(str).apply(lambda x: 'Shared' if 'hared' in x else ('Private' if 'rivate' in x else 'Normal'))\n",
    "df['bathrooms'] = df['bathrooms'].str.extract('(\\d+\\.?\\d*)').astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db760246-f5a5-41c8-bc84-59b16670f7da",
   "metadata": {},
   "source": [
    "Explanation: \n",
    "1. when we looked the data, we observe that the variable host_response_rate, host_acceptance_rate have the percentage sign, and price has the dollar sign. We extract numerical values by removing the percentage sign and dollar sign.\n",
    "2. Then we observe for bathroom feature, some value recorded as half/Half, so we convert the text to a number 0.5 and eliminate text for the bathroom column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1f8889-d2d1-4afa-b536-45ea94eac414",
   "metadata": {},
   "source": [
    "**STEP 2: Clean all features that contains multiple items of information, e.g. creating email, phone, work_email, etc. from feature host_verifications.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd093d9-2106-41d7-938f-cbf51e8285d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating 4 new features from host_verification feature\n",
    "df['email_verified'] = df['host_verifications'].apply(lambda x: 'email' in x).astype(int)\n",
    "df['phone_verified'] = df['host_verifications'].apply(lambda x: 'phone' in x).astype(int)\n",
    "df['work_email_verified'] = df['host_verifications'].apply(lambda x: 'work_email' in x).astype(int)\n",
    "df['num_verifications'] = df['host_verifications'].apply(lambda x: len(x.split(',')) if x else 0)\n",
    "# print(df['host_verifications'])\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e3b5f6-f8ed-4acc-87a6-2767bdae2359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split amenities into separate values and create a new row for each value\n",
    "amenities = df['amenities'].str.split(', ')\n",
    "amenities = amenities.explode()\n",
    "\n",
    "# count the frequency of each value\n",
    "value_counts = amenities.value_counts()\n",
    "# print the top 5 most frequent items\n",
    "print(value_counts.head(5))\n",
    "\n",
    "df['has_Kitchen'] = df['amenities'].str.contains('Kitchen', regex=False).astype(int)\n",
    "df['has_Smoke alarm'] = df['amenities'].str.contains('Smoke alarm', regex=False).astype(int)\n",
    "df['Hangers'] = df['amenities'].str.contains('Hangers', regex=False).astype(int)\n",
    "df['Wifi'] = df['amenities'].str.contains('Wifi', regex=False).astype(int)\n",
    "df['Iron'] = df['amenities'].str.contains('Iron', regex=False).astype(int)\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2043e1-33d3-4497-8d59-31874247c442",
   "metadata": {},
   "source": [
    "Explanation: \n",
    "\n",
    "When we observe the dataset, we observe the **host_verification and amenities** holding mulitple items. For the host_verification, I divided the 3 verifications to different features as dummy variable. As for the amenities feature has a lot more items, so I select the top 5 most frequent items in the amenities and divide them into different features as dummy variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0450e9f7-1c8c-4a8a-b4ad-bcb7a2431d32",
   "metadata": {},
   "source": [
    "**STEP 3: Impute missing values for all features in both training and test datasets.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9eae6f-c784-4c02-afdf-830ee085fa19",
   "metadata": {},
   "source": [
    "**Checking missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644940f2-5fad-4ac7-9101-58fcbd4554f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No. of null value in each variable\\n',df.loc[:, 'ID':'reviews_per_month'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2876406f-4c01-4e18-9a7f-91d326dfac60",
   "metadata": {},
   "source": [
    "**Addressing missing values**\n",
    "- **Handling location**\n",
    "\n",
    "('neighbourhood', 'neighbourhood_cleansed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db432b3-f3ab-44b2-a962-cf6c47dec9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean suburb to matches neighbourhood cleansed \n",
    "df['municipality'] = df['municipality'].str.replace('City of ', '')\n",
    "df['municipality'] = df['municipality'].str.replace('Shire of ', '')\n",
    "df['neighbourhood'] = df['suburb'] #makes the neighbourhood  column = suburb\n",
    "df['neighbourhood_cleansed'].fillna(df['municipality'], inplace=True) #use the municipality to replace na values for neighbourhood_cleansed\n",
    "# print('No. of null value in each variable\\n',df.loc[:, 'ID':'reviews_per_month'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41247335-f63d-461d-a2ba-83add560d492",
   "metadata": {},
   "source": [
    "- **Handling text variables**\n",
    "\n",
    "('name', 'description', 'neighborhood_overview', 'host_about')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73251231-2ea5-4b4c-88ed-60510b40867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing value in name, description, neighborhood_overview, and host_about with 'no information'\n",
    "df['name'] = df['name'].fillna('no information')\n",
    "df['description'] = df['description'].fillna('no information')\n",
    "df['neighborhood_overview'] = df['neighborhood_overview'].fillna('no information')\n",
    "df['host_about'] = df['host_about'].fillna('no information')\n",
    "# print('No. of null value in name variable\\n',df['name'].isnull().sum())\n",
    "# print('No. of null value in description variable\\n',df['description'].isnull().sum())\n",
    "# print('No. of null value in neighborhood_overview variable\\n',df['neighborhood_overview'].isnull().sum())\n",
    "# print('No. of null value in host_about variable\\n',df['host_about'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d54204-4a24-41cf-9995-f3cf5c6ca084",
   "metadata": {},
   "source": [
    "- **Fill missing values in categorical variables**\n",
    "\n",
    "('host_response_time', 'host_is_superhost','host_location', 'host_neighbourhood', 'property_type', 'room_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d0a232-10f6-4b49-94ea-e07637b6c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the training dataset and test dataset\n",
    "df_train_clean = df.iloc[:7000]\n",
    "# print(df_train_clean)\n",
    "df_test_clean = df.iloc[7000:]\n",
    "# print(df_test_clean)\n",
    "\n",
    "\n",
    "# Fill in the missing values in categorical variables with the most frequent values\n",
    "from scipy.stats import mode\n",
    "columns_to_process = ['host_response_time', 'host_is_superhost','host_location', 'host_neighbourhood', 'property_type', 'room_type']\n",
    "\n",
    "modes = df_train_clean[columns_to_process].mode(axis=0)\n",
    "for column in columns_to_process:\n",
    "    df[column].fillna(modes[column][0], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fdba24-bfa9-4f47-89f6-6438d6bf1809",
   "metadata": {},
   "source": [
    "- **Fill missing values in numerical variables**\n",
    "\n",
    "('host_response_rate', 'host_acceptance_rate', 'bathrooms', 'bedrooms', 'beds', 'minimum_minimum_nights', 'maximum_maximum_nights', 'availability_365', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'reviews_per_month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cb537a-9299-4945-89a3-bfd9c5899a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the missing values in numerical variables with the mean value\n",
    "columns_to_fill_mean = ['host_response_rate', 'host_acceptance_rate', 'bathrooms', 'bedrooms', 'beds', 'minimum_minimum_nights', 'maximum_maximum_nights', 'availability_365', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'reviews_per_month']\n",
    "\n",
    "for column in columns_to_fill_mean:\n",
    "    df[column].fillna(df_train_clean[column].mean(), inplace = True)   \n",
    "# print('No. of null value in each variable\\n',df.loc[:, 'ID':'reviews_per_month'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66ab540-3886-4775-9179-48c7fba35a2a",
   "metadata": {},
   "source": [
    "- **FIll missing values for date type** \n",
    "\n",
    "('first_review', 'last_review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218986f9-ef9c-42f6-aa47-a160e6ba384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date column to datetime type\n",
    "df['first_review'] = pd.to_datetime(df['first_review'])\n",
    "df['last_review'] = pd.to_datetime(df['last_review'])\n",
    "# impute missing values with the following date value\n",
    "df['first_review'] = df['first_review'].fillna(method='ffill')\n",
    "df['last_review'] = df['last_review'].fillna(method='ffill')\n",
    "# print(df['first_review'])\n",
    "\n",
    "# print('No. of null value in date variable\\n',df['first_review'].isnull().sum())\n",
    "\n",
    "# print('No. of null value in each variable\\n',df.loc[:, 'ID':'reviews_per_month'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2038cc-b071-4294-a548-76342dbeb84e",
   "metadata": {},
   "source": [
    "Explanation: \n",
    "\n",
    "##### Addressing missing values\n",
    "- **Handling location** - Geopy\n",
    "\n",
    "('neighbourhood', 'neighbourhood_cleansed')\n",
    "- **Handling text variables** - fill in 'no informaiton'\n",
    "\n",
    "('name', 'description', 'neighborhood_overview', 'host_about'\n",
    "- **Fill missing values in categorical variables** - using most frequent values in training dataset \n",
    "\n",
    "('host_response_time', 'host_is_superhost','host_location', 'host_neighbourhood', 'property_type', 'room_type')\n",
    "- **Fill missing values in numerical variables** - using mean values in training dataset\n",
    "\n",
    "('host_response_rate', 'host_acceptance_rate', 'bathrooms', 'bedrooms', 'beds', 'minimum_minimum_nights', 'maximum_maximum_nights', 'availability_365', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'reviews_per_month')\n",
    "- **FIll missing values for date type** - same value as the followed non-null value\n",
    "\n",
    "('first_review', 'last_review')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba1bdb1-1bec-4530-8737-d02187a4eeec",
   "metadata": {},
   "source": [
    "**STEP 4: Encode all categorical variables appropriately.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bccdc8-0dfc-455e-9cc8-980f4d944f49",
   "metadata": {},
   "source": [
    "Where a categorical feature contains more than 5 unique values, map the feature into 5 most frequent values + 'other' and then encode appropriately. For instance, you could group then map `property_type` into 5 most frequent property types + 'other'  \n",
    "(2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556a21ac-69c5-4968-8ee1-76ff0c897c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Checking: \n",
    "print(df[['host_location', 'host_verifications', 'neighbourhood', 'property_type', 'host_neighbourhood','neighbourhood_cleansed', 'room_type', 'host_response_time']].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013d9572-0896-449e-aaad-c31d95ff60d4",
   "metadata": {},
   "source": [
    "#### Variables need to encode: \n",
    "**property_type, host_location, neighbourhood, host_neighbourhood, and neighbourhood_cleansed**\n",
    "- One-hot encoding\n",
    "Since these are nominal categorical variables merge all encoded variables to the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767dc6b8-4bd7-4bb3-90fd-b9ac5b83ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent = 5  # Number of most frequent values to keep\n",
    "\n",
    "# Get value counts of the property_type feature\n",
    "property_type_counts = df['property_type'].value_counts()\n",
    "\n",
    "# print(property_type_counts)\n",
    "\n",
    "# Select the top 5 values and 'other'\n",
    "top_values_property = property_type_counts.head(most_frequent).index.tolist()\n",
    "top_values_property.append('other')\n",
    "\n",
    "# print(top_values_property)\n",
    "\n",
    "df['property_type_mapped'] = df['property_type'].map(lambda x: x if x in top_values_property else 'other')\n",
    "\n",
    "# print(df['property_type_mapped'])\n",
    "\n",
    "# Perform one-hot encoding\n",
    "encoded_property = pd.get_dummies(df['property_type_mapped'], prefix='property_type')\n",
    "# print(encoded_property)\n",
    "\n",
    "\n",
    "\n",
    "# Get value counts of the host_location feature\n",
    "host_location_counts = df['host_location'].value_counts()\n",
    "\n",
    "# print(host_location_counts)\n",
    "\n",
    "# Select the top N values and 'other'\n",
    "top_values_host = host_location_counts.head(most_frequent).index.tolist()\n",
    "top_values_host.append('other')\n",
    "\n",
    "# print(top_values_host)\n",
    "\n",
    "df['host_location_mapped'] = df['host_location'].map(lambda x: x if x in top_values_host else 'other')\n",
    "\n",
    "# print(df['host_location_mapped'])\n",
    "\n",
    "# Perform one-hot encoding\n",
    "encoded_host = pd.get_dummies(df['host_location_mapped'], prefix='host_location')\n",
    "# print(encoded_host)\n",
    "\n",
    "\n",
    "\n",
    "# Get value counts of the neighborhood feature\n",
    "nbr_counts = df['neighbourhood'].value_counts()\n",
    "\n",
    "# print(nbr_counts)\n",
    "\n",
    "# Select the top N values and 'other'\n",
    "top_values_nbr = nbr_counts.head(most_frequent).index.tolist()\n",
    "top_values_nbr.append('other')\n",
    "\n",
    "# print(top_values_host)\n",
    "\n",
    "df['nbr_mapped'] = df['neighbourhood'].map(lambda x: x if x in top_values_nbr else 'other')\n",
    "\n",
    "# print(df['host_location_mapped'])\n",
    "\n",
    "# Perform one-hot encoding\n",
    "encoded_nbr = pd.get_dummies(df['nbr_mapped'], prefix='neighbourhood')\n",
    "# print(encoded_nbr)\n",
    "\n",
    "\n",
    "\n",
    "# Get value counts of the host_neighborhood feature\n",
    "host_nbr_counts = df['host_neighbourhood'].value_counts()\n",
    "\n",
    "# print(host_nbr_counts)\n",
    "\n",
    "# Select the top N values and 'other'\n",
    "top_values_host_nbr = host_nbr_counts.head(most_frequent).index.tolist()\n",
    "top_values_host_nbr.append('other')\n",
    "\n",
    "# print(top_values_host_nbr)\n",
    "\n",
    "df['host_nbr_mapped'] = df['host_neighbourhood'].map(lambda x: x if x in top_values_host_nbr else 'other')\n",
    "\n",
    "# print(df['host_nbr_mapped'])\n",
    "\n",
    "# Perform one-hot encoding\n",
    "encoded_host_nbr = pd.get_dummies(df['host_nbr_mapped'], prefix='host_neighbourhood')\n",
    "# print(encoded_host_nbr)\n",
    "\n",
    "\n",
    "\n",
    "# Get value counts of the neighbourhood_cleansed feature\n",
    "nbr_cln_counts = df['neighbourhood_cleansed'].value_counts()\n",
    "\n",
    "# print(nbr_cln_counts)\n",
    "\n",
    "# Select the top N values and 'other'\n",
    "top_values_nbr_cln = nbr_cln_counts.head(most_frequent).index.tolist()\n",
    "top_values_nbr_cln.append('other')\n",
    "\n",
    "# print(top_values_nbr_cln)\n",
    "\n",
    "df['nbr_cln_mapped'] = df['neighbourhood_cleansed'].map(lambda x: x if x in top_values_nbr_cln else 'other')\n",
    "\n",
    "# print(df['nbr_cln_mapped'])\n",
    "\n",
    "# Perform one-hot encoding\n",
    "encoded_nbr_cln = pd.get_dummies(df['nbr_cln_mapped'], prefix='neighbourhood_cleansed')\n",
    "# print(encoded_nbr_cln)\n",
    "\n",
    "\n",
    "# merge all encoded variables to the original dataset\n",
    "df = pd.concat([df, encoded_property, encoded_host, encoded_nbr, encoded_host_nbr, encoded_nbr_cln], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fec9a9e-f4fa-4cd5-8fe1-2f8496f9bb8e",
   "metadata": {},
   "source": [
    "**STEP 5: Create dummy variables on amenities feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc665d13-3fba-41c2-95ce-3335c43c5488",
   "metadata": {},
   "outputs": [],
   "source": [
    "Intuitively, we think the balcony, private_balcony, courtyard_view, vineyard_view, TV and AC are affecting the price.\n",
    "\n",
    "Also, the bathroom types can influence the price. Three bathroom types defined previously (normal bathroom, private bathroom, public bathroom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c46591f-3372-41ce-b9bb-bbc27146d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many items for each Airbnb service\n",
    "amenities_count = df['amenities'].str.split(', ').apply(len)\n",
    "df['amenities_count'] = amenities_count\n",
    "print(amenities_count)\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b074399-3253-4316-ad32-eedc3d60e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select some variable items from amenities in each Airbnb house and make dummy variables\n",
    "df['has_balcony'] = df['amenities'].str.contains('patio or balcony|Patio or balcony', case=False, regex=True).astype(int)\n",
    "df['has_private_balcony'] = df['amenities'].str.contains('Private patio or balcony', case=False, regex=True).astype(int)\n",
    "df['has_courtyard_view'] = df['amenities'].str.contains('Courtyard view', case=False, regex=True).astype(int)\n",
    "df['has_vineyard_view'] = df['amenities'].str.contains('Vineyard view', case=False, regex=True).astype(int)\n",
    "df['has_TV'] = df['amenities'].str.contains('TV', case=False, regex=False).astype(int)\n",
    "df['has_AC'] = df['amenities'].str.contains('AC|Air conditioning', case=False, regex=True).astype(int)\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c3c6da-06cc-41c4-b36d-f9efed5b0189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode each bath type (normal bath, private bath, shared bath)\n",
    "pd.get_dummies(df['Bath Type'], prefix='Bath Type')\n",
    "encoded_bath_type = pd.get_dummies(df['Bath Type'], prefix='Bath Type')\n",
    "print(encoded_bath_type)\n",
    "\n",
    "df = pd.concat([df, encoded_bath_type], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3582bc5c-d027-42b0-800d-bae8f2542d09",
   "metadata": {},
   "source": [
    "**STEP 6: Perform exploratory data analysis to measure the relationship between the features and the target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d606d5d5-88ec-42e6-bee1-8d1a6ba6bc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.dtypes.unique())\n",
    "df_clean = df.select_dtypes(include=['int64', 'float64', 'int32'])\n",
    "# print(df_clean)\n",
    "\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df_clean.corr()\n",
    "\n",
    "# Filter features based on correlation threshold\n",
    "\n",
    "# Print the strongly correlated features\n",
    "print(abs(correlation_matrix['price']).sort_values(ascending=False).head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077601e2-846a-42d2-84e4-d004417f2c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter features based on correlation threshold\n",
    "correlation_threshold = 0.015  # Adjust the threshold as per your requirement\n",
    "strong_correlated_features = correlation_matrix[abs(correlation_matrix['price']) > correlation_threshold]['price']\n",
    "\n",
    "# Print the strongly correlated features\n",
    "print(abs(strong_correlated_features).sort_values(ascending=False))\n",
    "\n",
    "# Select the strong correlated features and use them as input for traning and testing\n",
    "df_final = df_clean[['bedrooms','accommodates', 'minimum_minimum_nights', 'bathrooms', 'beds', 'availability_365', 'minimum_nights_avg_ntm', 'availability_30', 'reviews_per_month', 'maximum_nights', 'minimum_nights', 'email_verified', 'maximum_minimum_nights', 'has_TV', 'number_of_reviews_ltm', 'num_verifications', 'number_of_reviews','number_of_reviews_l30d','price']]\n",
    "\n",
    "df_train_final = df_final.iloc[:7000]\n",
    "# print(df_train_final)\n",
    "df_test_final = df_final.iloc[7000:]\n",
    "# print(df_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390923cd-7a6f-4934-a9c3-8ff5f6ef6e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Calculate correlations between selected features and the target\n",
    "correlation_matrix_plot = df_final.corr()\n",
    "plt.figure(figsize = (15,8))\n",
    "sns.heatmap(correlation_matrix_plot, annot=True, cmap='coolwarm', vmin = -0.1, vmax = 0.1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c8bab3-e7a1-4f14-a0ab-f46ee14c1205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.pairplot(df_final)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7753a260-2e7d-48e9-8041-de5794045a45",
   "metadata": {},
   "source": [
    "We tried to select all numerical variables which has int64, float32, int32 and uint8. We iterate through data cleaning, feature engineering, and exploration steps to refine the dataset. After tried different errors, We found that the variables with int64, float32 and int32 with variables correlation greater than 0.015 gave us the best result. The resulting dataset will be used for training and evaluating the predictive models.\n",
    "\n",
    "Selected features as follows: ('bedrooms','accommodates', 'minimum_minimum_nights', 'bathrooms', 'beds', 'availability_365', 'minimum_nights_avg_ntm', 'availability_30', 'reviews_per_month', 'maximum_nights', 'minimum_nights', 'email_verified', 'maximum_minimum_nights', 'has_TV', 'number_of_reviews_ltm', 'num_verifications', 'number_of_reviews','number_of_reviews_l30d')\n",
    "\n",
    "Those features do not present a strong linear relationship"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
